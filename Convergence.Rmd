---
title: "Convergence testing"
output: html_notebook
---

Load in the fit framework. Converting to a function makes it easier to work with here/

```{r}
library(tidyverse)
library(BART)
library(caret)

source("generate_noise.R")

get_bart_estims <- function(data.p, data.py) {
  data_py.raw <- data.py
  data_p.raw <- data.p
  
  # Format data -------------------------------------------------------------
  
  data_py.wide <- data_py.raw %>%
    pivot_longer(!c(id.practice, year, Z)) %>%
    group_by(id.practice, name) %>%
    arrange(id.practice, name, year) %>% 
    mutate(diff1 = value - lag(value),
           diff2 = value - lag(lag(value)),
           diff3 = value - lag(lag(lag(value)))) %>% 
    ungroup() %>%
    pivot_wider(id_cols = c(id.practice, Z),
                names_from = c(name, year),
                values_from = c(value, diff1, diff2, diff3),
                names_glue = "{name}{ifelse(.value=='value','','_diff')}.y{year}{ifelse(.value=='value','',paste0('_y',year-parse_number(.value,na='value')))}") %>% 
    select(!starts_with("post") & 
             !ends_with(c('0', '-1', '-2')) & 
             !starts_with(c("Y_diff.y3", "Y_diff.y4"))) %>% 
    mutate(Y_avg.post = (Y.y3 * n.patients.y3 + Y.y4 * n.patients.y4) /
             (n.patients.y3 + n.patients.y4)) %>%
    select(-Y.y4, -Y.y3)
  
  data.main <- inner_join(data_py.wide, data_p.raw, by = "id.practice")
  class(data.main) <- "data.frame"
  
  rm(data_py.raw, data_p.raw, data_py.wide)
  
  
  # Fit BART/DART to data+noise ---------------------------------------------
  
  x_names <- setdiff(names(data.main), c("Y_avg.post", 'id.practice'))
  
  # TODO: we need longer burn-in for bart, and especially dart
  # 100->200 for bart
  # for dart, needs iteration
  # skip, don't take more samples
  bart.fit <- gbart(data.main[x_names], data.main[['Y_avg.post']], sparse = F, nskip = 200)
  dart.fit <- gbart(data.main[x_names], data.main[['Y_avg.post']], sparse = T, nskip = 2000, ndpost = 1000, keepevery = 5)
  
  
  # Partial dependence calculations -----------------------------------------
  
  pdep <- function(.model, .data) {
    newdata.z1 <- newdata.z0 <- .data %>% 
      select(!c(Y_avg.post, id.practice)) %>%
      bartModelMatrix()
    newdata.z0[,'Z'] <- 0
    newdata.z1[,'Z'] <- 1
    
    pred.z0 <- predict(.model, newdata.z0)
    pred.z1 <- predict(.model, newdata.z1)
    
    pred.diff <- pred.z1 - pred.z0
    return(pred.diff)
    
  }
  
  posterior_subgroup_avgs <- function(pred.diff, .data) {
    #pred.diff <- pdep(.model, .data)
    
    # TODO: debug this line
    if (is.null(dim(pred.diff))) return(pred.diff)
    
    # Use total number of patients as weights
    n.patients <- .data %>% 
      # TODO: fix this line
      select(n.patients.y3, n.patients.y4) %>% 
      rowSums()  
    
    # Take the weighted average of subgroup
    # Returns a vector of length = # posterior draws
    return(apply(pred.diff, 1, function(.) weighted.mean(., n.patients)))
  }
  
  posterior_estimate <- function(pred.diff, .data) {
    pavg <- posterior_subgroup_avgs(pred.diff, .data)
    
    estimate <- mean(pavg)
    # Using a 90% credible interval
    interval <- quantile(pavg, c(.05, .95))
    
    #return(list(estimate = estimate, interval = interval))
    return(data.frame(est_satt = estimate, q05 = interval[[1]], q95 = interval[[2]]))
  }
  
  get_all_estimates <- function(pred.diff, .data) {
    # Because .data is masked in dplyr functions
    ..data <- .data
    
    satt.overall <- posterior_estimate(pred.diff, ..data) %>% 
      mutate(variable = "Overall")
    
    # We don't have a satt.yearly because we analyzed a wide dataset with no time
    # groups
    
    satt.practice <- list_rbind(map(unique(..data$id.practice), function(.x) {
      ..data.sub <- ..data %>% mutate(.idx = 1:nrow(..data)) %>% filter(id.practice == .x)
      posterior_estimate(pred.diff[, ..data.sub$.idx], ..data.sub) %>% 
        mutate(id.practice = .x)
    }))
    
    satt.levels <- list_rbind(map(1:5, function(.x)
      list_rbind(map(unique(..data[[paste0("X", .x)]]), function(.y) {
        ..data.sub <- ..data %>% 
          mutate(.idx = 1:nrow(..data)) %>% 
          filter(!!sym(paste0("X", .x)) == .y)
        posterior_estimate(pred.diff[, ..data.sub$.idx], ..data.sub) %>% 
          mutate(variable = paste0("X", .x), level = as.character(.y))
      })) 
    ))
    
    return(bind_rows(satt.overall, satt.levels, satt.practice))
    
  }
  
  
  # Run performance functions -----------------------------------------------
  
  # Run the performance functions for BART
  pdep_draws.bart <- pdep(bart.fit, data.main)
  bart.ests <- get_all_estimates(pdep_draws.bart, data.main)
  
  # Run the performance functions for DART
  pdep_draws.dart <- pdep(dart.fit, data.main)
  dart.ests <- get_all_estimates(pdep_draws.dart, data.main)
  
  
  # Output posterior draws and estimates ------------------------------------
  
  return(list(
    BART = list(
      pdep = pdep_draws.bart,
      ests = bart.ests,
      fit = bart.fit
    ),
    DART = list(
      pdep = pdep_draws.dart,
      ests = dart.ests,
      fit = dart.fit
    )
  ))
  
}
# Performance evals in another script/notebook, combined with curia evals &c.

```

Load our data

```{r}
path_p <- "Data/track2_20220404/practice/acic_practice_%s.csv"
path_py <- "Data/track2_20220404/practice_year/acic_practice_year_%s.csv"

data_p.raw <- read.csv(sprintf(path_p, "0001"), stringsAsFactors = TRUE)
data_py.raw <- read.csv(sprintf(path_py, "0001"), stringsAsFactors = TRUE)

```

Generate some test noise datasets

```{r}
data_p.noise10 <- add_noise_cols(data_p.raw, 10, var_prefix = "noise_P")
data_p.noise100 <- add_noise_cols(data_p.raw, 100, var_prefix = "noise_P")
data_p.noise1000 <- add_noise_cols(data_p.raw, 1000, var_prefix = "noise_P")

data_py.noise10 <- add_noise_cols(data_py.raw, 10, var_prefix = "noise_PY")
data_py.noise100 <- add_noise_cols(data_py.raw, 100, var_prefix = "noise_PY")
data_py.noise1000 <- add_noise_cols(data_py.raw, 1000, var_prefix = "noise_PY")

```

Now create fit objects

```{r message=FALSE, cache=TRUE, include=FALSE}
fittime <- system.time({
  bart0 <- get_bart_estims(data_p.raw, data_py.raw)
  bart10 <- get_bart_estims(data_p.noise10, data_py.noise10)
  bart100 <- get_bart_estims(data_p.noise100, data_py.noise100)
  bart1000 <- get_bart_estims(data_p.noise1000, data_py.noise1000)
})

```

```{r}
fittime/60

```

Analyze convergence properties of fit objects using code below

```{r}
test_convergence_plots <- function(.pdep, .fit, .burnin = NULL) {
  
    oldpar <- par(no.readonly = TRUE)
    on.exit(par(oldpar), add = TRUE)
    par(mfrow = c(2, 2))
    #plot.new()
    
    z <- gewekediag(.pdep)$z
    qqnorm(z)
    abline(0,1, col='red')
    title("PDep Geweke QQ")
    
    plot(.pdep[,which.min(z)], type='l')
    title("Trace Min Geweke")
    plot(.pdep[,which.max(z)], type='l')
    title("Trace Max Geweke")
    
    plot(.fit$sigma, type = 'l')
    if (!is.null(.burnin)) abline(v = .burnin, col = 'red')
    title("Trace Sigma")
    
}

test_convergence_plots(bart0$BART$pdep, bart0$BART$fit)
test_convergence_plots(bart0$DART$pdep, bart0$DART$fit)

test_convergence_plots(bart10$BART$pdep, bart10$BART$fit)
test_convergence_plots(bart10$DART$pdep, bart10$DART$fit)

test_convergence_plots(bart100$BART$pdep, bart100$BART$fit)
test_convergence_plots(bart100$DART$pdep, bart100$DART$fit)

test_convergence_plots(bart1000$BART$pdep, bart1000$BART$fit)
test_convergence_plots(bart1000$DART$pdep, bart1000$DART$fit)
```

