---
title: "R Notebook"
output: html_notebook
---

# Setup/metadata

```{r}
library(tidyverse)

knitr::opts_chunk$set(message = FALSE)

```

Which datasets are we looking at? I.e., which datasets did Curia provide us with?

```{r message=FALSE}
dataset_nums <- read_csv("Data/curia_data/dataset_nums.csv") %>% 
  pull(dataset_num)
```

Ground truth datasets

```{r}
estimand_truths <- read_csv("Data/ACIC_estimand_truths.csv")

```

DGP descriptions. Output of chunk shows unique combinations in current set of dataset nums.

```{r}
dgp_levels <- estimand_truths %>% 
  select(dataset.num:`Idiosyncrasy of Impacts`) %>% 
  distinct()

dgp_levels %>% 
  filter(dataset.num %in% dataset_nums) %>% 
  distinct(pick(-dataset.num))

```



## Practice/Practice-Year Information

We'll want to use weighted means by number of patients in order to get a lot of subgroup estimates later down the road. 

First I load in practice year level data from the ACIC competition, select just the year and the number of patients in each practice, and create a date set number column by Rex parsing the file names. Then I bind them all together into a single large data set that has the number of patients per practice per year for all ACIC data sets.

Next I map overall ACIC practice level data sets, load in the data set for each, and similarly parse the file name into a data set number. This gives me a single data frame containing the practice level characteristics of each practice, which will be needed to join on data set later for analysis.

Lastly, because this is an expensive operation, I save out both data sets so that they can be loaded directly later on. Unless this chunk is modified, it will be skipped for evaluation and the next chunk which loads the data sets, will be run.

```{r, eval=FALSE}
# Read in the competition data to get practice-year-level data for weighted means
n.patients.all <- dir("Data/track2_20220404/practice_year/", full.names = TRUE) %>% 
  map(~ read_csv(.x) %>% 
        select(id.practice, n.patients, year) %>% 
        mutate(dataset.num = str_match(basename(.x), '[0-9]+')[1,1])) %>% 
  list_rbind()

# Read in the competition data to get practice-level data for weighted means
practice_level <- dir("Data/track2_20220404/practice/", full.names = TRUE) %>% 
  map(~ read_csv(.x) %>% 
        mutate(dataset.num = str_match(basename(.x), '[0-9]+')[1,1])) %>% 
  list_rbind()

write_csv(practice_level, "Data/formatted/practice_true.csv")
write_csv(n.patients.all, "Data/formatted/n_patients_all.csv")

```

Read in the cached results of the previous chunk.

```{r, message=FALSE}
# Load results of previous chunk
practice_level <- read_csv("Data/formatted/practice_true.csv")
n.patients.all <- read_csv("Data/formatted/n_patients_all.csv")

```

The subgroups we are interested in are those formed by the different levels of the categorical X variables at the practice level, as well as by practice, by year, and overall. Because these are overlapping subgroups, we are going to be stacking them with `rbind` later down the road. This chunk prepares us for this by similarly stacking number of patients so that we can join them on an analysis status with the appropriate subgroups. 

The output of this chunk shows in a bridged version of the resulting data frame.

```{r}
# aggregate and summarize by all the variables
n.patients.overall <- n.patients.all %>% 
  group_by(dataset.num) %>% 
  summarize(n.patients = sum(n.patients), variable = "Overall")

n.patients.by_practice <- n.patients.all %>% 
  group_by(dataset.num, id.practice) %>% 
  summarise(n.patients = sum(n.patients))

n.patients.by_year <- n.patients.all %>% 
  group_by(dataset.num, year) %>% 
  summarise(n.patients = sum(n.patients), variable = "Overall")

# Merge on practice level variables
n.patients.practice_level <- n.patients.all %>% 
  left_join(practice_level, by = c('id.practice', 'dataset.num'))

# summarize by level of covariates
n.patients.by_X <- map(1:5, function(i) {
  n.patients.practice_level %>% 
    group_by(dataset.num, !!sym(paste0("X", i))) %>% 
    pivot_longer(cols = c(!!sym(paste0("X", i))), names_to = 'variable', values_to = 'level') %>% 
    mutate(level = as.character(level)) %>% 
    select(id.practice, n.patients, year, dataset.num, variable, level)
}) %>% 
  list_rbind() %>% 
  group_by(dataset.num, variable, level) %>% 
  summarise(n.patients = sum(n.patients))

# Create one dataframe to merge
n.patients.combined <- bind_rows(n.patients.overall, n.patients.by_practice, 
                                 n.patients.by_X, n.patients.by_year)

# Output of this chunk shows an abridged version of what n.patients.combined looks like
n.patients.combined %>% 
  filter(dataset.num == '0001', 
         id.practice %in% c(NA, 1, 2, 3), 
         variable %in% c('X1', 'X2', NA, 'Overall'))
```


# Performance estimates

This section covers loading the competition results, formatting them, and computing performance estimates from them. We get performance estimates from three main sources:

- The ACIC 2024 competition results
- The estimates given to us by Curia
- BART/DART estimates (calculated in 'run_all_loops.R')

## ACIC winners

The ACIC contest results are provided in the form of a serialized R binary object. I create an object `acic_winners.df` which places the results of all the methods into a single dataframe.

```{r, cache=TRUE}
# acic_winners.raw <- readRDS("Data/eval_simple_tosend.RDS")
acic_winners <- readRDS("Data/eval_simple_tosend.RDS")

acic_winners.df <- names(acic_winners) %>% 
  #keep(~ grepl("^t2", .x)) %>% 
  map(~ acic_winners[[.x]]$eval %>% mutate(method = .x)) %>%
  list_rbind() %>% 
  select(-DGP_name) %>% 
  mutate(noise = FALSE,
         outcome= 'ywavg')

```


## Curia

Curia estimates are loaded first from the baseline data sets and then from the 10 X noise generated sets. Some amount of filtering is necessary to ensure we're only loading the files we want. Since this is a slow process, I'm also saving out the resulting data set for quick loading in the next chunk.

```{r, eval=FALSE, message=FALSE}
# Load in curia estimates from separate tables.
# ⚠️ Slow
curia_estimates <- dir("Data/curia_results/1x/", full.names = TRUE) %>%
  # grep("acic_[a-z]+_([0-9]{4})\\.csv", ., value=T) %>%
  map(~ read_csv(.x)) %>%
  list_rbind() %>%
  mutate(dataset.num = sprintf("%04d", dataset.num)) %>% 
  mutate(noise = FALSE)

curia_estimates_10x <- dir("Data/curia_results/satt_results_10x/", full.names = TRUE) %>%
  grep("acic_[a-z]+_([0-9]{4}b)\\.csv", ., value=T) %>% 
  map(~ read_csv(.x)) %>%
  list_rbind() %>% 
  mutate(dataset.num = sprintf("%04d", dataset.num)) %>% 
  mutate(noise = TRUE, outcome = 'ywavg')

curia_estimates <- bind_rows(curia_estimates, curia_estimates_10x)


# Write for quick loading
write_csv(curia_estimates, "Data/formatted/all_curia_estimates_10x.csv")

```

```{r, message=FALSE}
# Load results of previous chunk
curia_estimates <- read_csv("Data/formatted/all_curia_estimates_10x.csv")
   

```

### Performance metrics

Here, I'm generating some necessary precursors for performance analysis. First, the Curia estimates are joined with the ground truths from the ACC competition and the number of patients which will be used for weighted averages. In this step, I also subset down to just the data sets we will use for the comparison between curia and Bart (600 baseline, 200 10x noise).

After this, I calculate the bias, absolute bias, confidence interval width, interval boundaries, and interval coverage for each data point, and then summarizing within the relevant subgroups - variable and level, year, data set number, and baseline versus 10X noise - using number of patience as a wait for a weighted average.

```{r}
# Join on the ground truths
combined_ests <- curia_estimates %>% 
  left_join(estimand_truths, 
            by = c("dataset.num", "variable", "level", "year", "id.practice"),
            suffix = c(".curia", "")) %>% 
  select(!ends_with(".curia")) %>% 
  # Replace inner_join with left_join when running full scale on all dataset.num
  left_join(n.patients.combined, by = c("dataset.num", "variable", "level", "year", "id.practice")) %>% 
  filter(dataset.num %in% dataset_nums)

curia_eval.datasets <- combined_ests %>% 
  # Not estimating practice effects for this comparison
  filter(is.na(id.practice)) %>% 
  # Align names with scoring data from competition
  rename(true = SATT, satt = model_satt, se = model_se) %>% 
  mutate(bias = true - satt,
         abs_bias = abs(bias),
         # lower.90 = satt - qnorm(.95) * se,
         # upper.90 = satt + qnorm(.95) * se,
         # cover = between(true, lower.90, upper.90),
         width = qnorm(.95) * 2 * se) %>% 
  mutate(lower.90 = satt - qnorm(.95) * se,
         upper.90 = satt + qnorm(.95) * se,
         cover = between(true, lower.90, upper.90)) %>% 
  summarise(.by = c(variable, level, year, dataset.num, noise),
            # rmse = caret::RMSE(satt, true),
            across(c(true, satt, se, bias, abs_bias, width, lower.90, upper.90, cover), ~ weighted.mean(.x, n.patients))) %>% 
  mutate(id.practice = NA,
         outcome = 'ywavg',
         method = "curia")
```


## Bart

```{r, message=FALSE, eval=FALSE}
bart_satt_ests.l <- dir("Data/analyzed/run_all_loops/", full.names = TRUE) %>%
  keep(~ !file.info(.x)$isdir) %>% 
  keep(~ file.info(.x)$size > 10) %>% 
  grep('[0-9]{4}b?(_ydiff_w)?_ps', ., value = TRUE) %>% 
  map(function(x) {
    df <- read_csv(x)
    # tryCatch(read_csv(x), error = function(e) esc <<- TRUE)
    df$method <- sub('^.*[0-9]{4}(.*)\\.csv$', '\\1', x)
    df$dataset.num <- sub('.*([0-9]{4}).*', '\\1', x)
    df$level <- as.character(df$level)
    return(df)
  }) 
bart_satt_ests <- bart_satt_ests.l %>%
  list_rbind() #%>%
  #mutate(dataset.num = sprintf("%.04s", dataset.num))

saveRDS(bart_satt_ests, "Save/bart_satt_ests_600_1x_extended.rds")
#rm(bart_satt_ests.l)
```

```{r, message=FALSE}
# bart_satt_ests <- readRDS("Save/bart_satt_ests_600_1x.rds")
bart_satt_ests <- readRDS("Save/bart_satt_ests_600_1x_extended.rds") #%>% 
  #filter(grepl('^b_ps', method))
bart_satt_ests <- bart_satt_ests %>% 
  mutate(noise = grepl('^b', method),
         outcome = ifelse(grepl('^ydiff', method), 'ydiff', 'ywavg'),
         method = sub('^b?(_ydiff)?(_w)?_', '', method)) %>% 
  filter(grepl('ps', method)) %>% 
  #bind_rows(., read_csv("Data/analyzed/satt_ests_base.csv")) %>% 
  mutate(noise = replace_na(noise, FALSE))
  
```

```{r}
bart_perf <- bart_satt_ests %>% 
  # Merge on the truth
  left_join(estimand_truths %>% 
              filter(is.na(year)), 
            by = c("dataset.num", "variable", "level", "id.practice"),
            suffix = c(".bart", "")) %>% 
  select(!ends_with(".bart")) %>% 
  rename(lower.90 = q05, upper.90 = q95, satt = est_satt, true = SATT) %>% 
  left_join(n.patients.combined, by = c("dataset.num", "variable", "level", "year", "id.practice")) %>% 
  filter(dataset.num %in% dataset_nums) %>% 
  mutate(bias = true - satt,
         abs_bias = abs(bias),
         # TODO: vvv why comment this line out? vvv
         # cover = between(true, lower.90, upper.90),
         width = upper.90 - lower.90) %>% 
  summarise(.by = c(variable, level, year, dataset.num, method, noise, outcome),
            # TODO: vvv why comment this line out? vvv
            # rmse = caret::RMSE(satt, true),
            across(c(true, satt, bias:width, lower.90, upper.90), 
                   ~ weighted.mean(.x, n.patients))) %>% 
  # TODO: vvv Why calc this after summarize, instead of before? vvv
  mutate(cover = between(true, lower.90, upper.90)) %>% 
  mutate(id.practice = NA, se = NA)

  
```


# Merge the performance data

```{r}
# One large dataset for comparing all evals
evals_compare <- acic_winners.df %>% 
  # mutate(noise = FALSE) %>% 
  # select(-DGP_name) %>% 
  rbind(curia_eval.datasets) %>% 
  rbind(bart_perf) %>% 
  left_join(n.patients.combined, by = c("dataset.num", "variable", "id.practice", "level", "year")) %>% 
  select(-id.practice) %>% 
  filter(dataset.num %in% dataset_nums) %>% 
  # Merge back on DGP info
  left_join(dgp_levels, by = 'dataset.num')

evals_compare

```

```{r, eval=FALSE}
evals_compare %>% 
  filter(variable == 'Overall', is.na(year)) %>% 
  write_csv("Data/all_methods_overall.csv")

```


# Analysis and plots

```{r}
data.plot <- evals_compare %>% 
  # Summarize across datasets
  summarise(
    .by = c(variable, level, year, method, `Confounding Strength`, noise, outcome),
    subgroup_size = mean(n.patients, na.rm = TRUE),
    rmse = caret::RMSE(satt, true),
    across(c(bias, abs_bias, cover, width), ~weighted.mean(., n.patients))
  ) %>% 
  filter(is.na(year)) %>% 
  # Remove unfinished ACIC submissions
  filter(!(method %in% c('t1_rBARTimpT', 'ofpsbart', 't1_H-TMLE'))) %>%
  # Create sensible variables for visualization
  mutate(
    # So we can treat all subgroups with the same variable
    variable_level = paste0(variable, 
                            ifelse(is.na(level), '', paste0('_', level)),
                            ifelse(is.na(year), '', paste0('_y', year))),
    # Ensures methods of interest are at the top of the draw order
    # method = factor(method, levels = rev(c('curia', 'BART', 'DART', 
    #                                        setdiff(unique(method), 
    #                                                c('curia', 'BART', 'DART'))))),
    # method = factor(method, levels = rev(c('curia', 'BART', 'DART', '_ps_BART', '_ps_DART', '_pd_sd_BART', '_ps_sd_DART', '_ps_sd_2l_BART', '_ps_sd_2l_DART', setdiff(unique(method), c('curia', 'BART', 'DART', '_ps_BART', '_ps_DART', '_pd_sd_BART', '_ps_sd_DART', '_ps_sd_2l_BART', '_ps_sd_2l_DART'))))),
    # Sensible method labelling
    method_label = case_match(method, 
                              'curia' ~ 'curia', 
                              'BART' ~ 'BART',
                              'DART' ~ 'DART',
                              'ps_BART' ~ 'ps_BART',
                              'ps_DART' ~ 'ps_DART',
                              'ps_sd_BART' ~ 'ps_sd_BART',
                              'ps_sd_DART' ~ 'ps_sd_DART',
                              'ps_sd_2l_BART' ~ 'ps_sd_2l_BART',
                              'ps_sd_2l_DART' ~ 'ps_sd_2l_DART',
                              .default = 'other'),
    variant = ifelse(noise, '10x', ifelse(outcome == 'ydiff', 'ydiff', 'baseline'))
  )

# data.plot

# method_colors <- c(Curia = "#F8766D", Oracle = "#00BFC4", Other = "darkgrey", 
#                    BART = '#4356FF', DART = '#AB54F9')
method_colors <- c(Curia = "#F8766D", Oracle = "#00BFC4", Other = "darkgrey", 
                   `ps_BART` = '#4356FF', `ps_DART` = '#AB54F9',
                    BART = "yellow", DART = "orange",
                   `ps_sd_BART` = '#43569F', `ps_sd_DART` = '#AB5499',
                   `ps_sd_2l_BART` = '#43563F', `ps_sd_2l_DART` = '#AB5439')
method_colors2 <- c(curia = "#F8766D", oracle = "#00BFC4", other = "darkgrey", 
                    BART = "yellow", DART = "orange",
                   `ps_BART` = '#4356FF', `ps_DART` = '#AB54F9',
                   `ps_sd_BART` = '#93b69F', `ps_sd_DART` = '#fB0499',
                   `ps_sd_2l_BART` = '#43f63F', `ps_sd_2l_DART` = '#AB5439')


```

```{r}
ggplot(mapping = aes(group = method, x = subgroup_size, y = rmse, color = method_label)) + 
  geom_line(data = filter(data.plot, method_label == 'other')) +
  geom_line(data = filter(data.plot, method_label != 'other', noise == FALSE, outcome == 'ywavg')) +
  geom_line(data = filter(data.plot, method_label != 'other', noise == TRUE), linetype = 2) +
  geom_line(data = filter(data.plot, method_label != 'other', noise == FALSE, outcome == 'ydiff'), linetype = 3) +
         # geom_line(aes(group = method, color = method_label)) +
  # scale_color_manual(values = c(curia = 'darkblue', BART = 'darkred', DART = 'cyan', other = 'lightgray')) +
  scale_color_manual(values = method_colors2) +
  #scale_linetype_identity(labels = c('10x', 'ydiff', 'base'), breaks = c(2, 3, 1), guide = guide_legend()) +
  facet_wrap(~ `Confounding Strength`) +
  scale_y_log10()
```

```{r, eval=FALSE}
evals_compare %>% 
         summarise(.by = c(variable, level, year, method),
                   subgroup_size = mean(n.patients),
                   rmse = caret::RMSE(satt, true), 
                   across(c(satt, true, bias, abs_bias, lower.90, upper.90, cover, width),
                          ~ weighted.mean(., n.patients))) %>%
        write_csv("Data/eval_compare_10x.csv")
```

```{r}
#method_groups = c('curia', keep(names(acic_winners), ~grepl("^t2", .)))

map(c("rmse", "bias", "abs_bias", "cover", "width"), function(.x) {
  ggplot(mapping = aes(x = subgroup_size, y = !!sym(.x), group = method, color = method_label)) + 
    geom_line(data = filter(data.plot, method_label == 'other')) +
    geom_line(data = filter(data.plot, method_label != 'other', noise == FALSE, outcome == 'ywavg')) +
    geom_line(data = filter(data.plot, method_label != 'other', noise == TRUE), linetype = 2) +
    geom_line(data = filter(data.plot, method_label != 'other', noise == FALSE, outcome == 'ydiff'), linetype = 3) +
    # scale_color_manual(values = c(curia = 'darkblue', BART = 'darkred', DART = 'cyan', other = 'lightgray')) +
    scale_color_manual(values = method_colors2) +
    facet_wrap(~ `Confounding Strength`) +
    {if(.x %in% c("rmse")) scale_y_log10() else NULL} + 
    labs(title = .x)
  ggsave(file = paste0("Plots/plot_", .x, "_by_subgroup_size_10x.png"))
})

```

```{r}
# Mare's plot_perf code
perf <- data.plot %>%
  filter(!(method %in% c('t1_rBARTimpT', 'ofpsbart', 't1_H-TMLE'))) %>%
  filter(variable=='Overall' & is.na(year))

# perf <- rbind(perf, rep(NA, ncol(perf)))
# perf$method[nrow(perf)] <- 'Oracle'
# perf$rmse[nrow(perf)] <- 0
# perf$width[nrow(perf)] <- 0
# perf$cover[nrow(perf)] <- .9

oracle_row <- data.frame(
  method = "Oracle",
  rmse = 0,
  width = 0,
  cover = 0.9,
  variable = "Overall",
  bias = 0,
  abs_bias = 0
)
perf <- bind_rows(perf, oracle_row) %>% 
  mutate(Method = case_match(method, 
                              'curia' ~ 'Curia', 
                              'Oracle' ~ 'Oracle',
                              'BART' ~ 'BART',
                              'DART' ~ 'DART',
                              'ps_BART' ~ 'ps_BART',
                              'ps_DART' ~ 'ps_DART',
                              'ps_sd_BART' ~ 'ps_sd_BART',
                              'ps_sd_DART' ~ 'ps_sd_DART',
                              'ps_sd_2l_BART' ~ 'ps_sd_2l_BART',
                              'ps_sd_2l_DART' ~ 'ps_sd_2l_DART',
                              .default = 'Other'),
         noise = ifelse(is.na(noise), FALSE, noise))
  # mutate(Method = case_match(
  #   method,
  #   'curia' ~ 'Curia',
  #   'Oracle' ~ 'Oracle',
  #   'BART' ~ 'BART',
  #   'DART' ~ 'DART',
  #   .default = "Other"
  # ))

# perf <- perf %>%
#   mutate(Method = ifelse(method == 'curia', 'Curia', 
#                          ifelse(method == 'Oracle', 'Oracle', 'Other')))

# method_colors <- c(Curia = "#F8766D", Oracle = "#00BFC4", Other = "darkgrey")
# method_colors <- c(Curia = "#F8766D", Oracle = "#00BFC4", Other = "darkgrey", 
#                    BART = '#4356FF', DART = '#AB54F9')


#meta <- readRDS('real_submissions.RDS')

curia_rmse <- perf %>%
  filter(Method=='Curia') %>%
  pull(rmse)

perf %>%
  summarise(percent_better = mean(rmse < curia_rmse, na.rm=T))

perf %>%
  ggplot(aes(x=rmse, y=cover, color=Method, shape=variant)) + 
  geom_hline(yintercept = 0.9, color='darkgrey') +
  geom_point() + 
  xlab('RMSE, overall SATT') +
  ylab('Coverage of overall SATT 90% interval') +
  facet_wrap(~ `Confounding Strength`) +
  scale_color_manual(values=method_colors)
ggsave(file.path('Plots', 'rmse_cover.png'), height=3.5, width=4.5)

perf %>%
  ggplot(aes(x=width, y=cover, color=Method, shape = variant)) + 
  geom_hline(yintercept = 0.9, color='darkgrey')+
  geom_point() + 
  xlab('Width of overall SATT 90% interval') +
  ylab('Coverage of overall SATT 90% interval') +
  facet_wrap(~ `Confounding Strength`) +
  scale_color_manual(values=method_colors)
ggsave(file.path("Plots", 'width_cover_10x.png'), height=3.5, width=4.5)

```

```{r}
data.plot %>% filter()
```

